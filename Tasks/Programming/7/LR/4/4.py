# -*- coding: utf-8 -*-
"""lab4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LZJvJXveHuFi0ms3DSauA2dPg8CrK1R2
"""

import requests
from bs4 import BeautifulSoup

DOMAIN = 'https://www.herzen.spb.ru'
ATLAS_DOMAIN = 'https://atlas.herzen.spb.ru/'

def get_institutes_list():
    # получение списка институтов со страницы https://www.herzen.spb.ru/main/structure/inst/
    html = requests.get(DOMAIN + '/main/structure/inst/').text # получение кода страницы

    institutes_list = [] # список институтов

    bs = BeautifulSoup(html, 'html.parser') # инициализация BeautifulSoup для парсинга страницы
    nameList = bs.find('td', {'class': 'block'}) # ссылки на страницы институтов содержатся в таблице в левой части страницы
    for el in nameList.ul.findAll('li'): # перебираем все элементы (li) списка (ul)
        # текст ссылки - название института, href - ссылка на страницу
        data = {'institute_name': el.a.text, 'url': DOMAIN + el.a.get('href')}
        institutes_list.append(data) # добавление информации об институте в список

    return institutes_list


def get_head_info(link):
    # получение информации о заведующем кафедры из Атласа
    # например, https://atlas.herzen.spb.ru/chair_type.php?id=294
    html = requests.get(ATLAS_DOMAIN + link).text # получение кода страницы

    data = dict() # словарь, который будет содержать ФИО и e-mail зав. кафедры
    
    bs = BeautifulSoup(html, 'html.parser') # инициализация BeautifulSoup для парсинга страницы
    title = bs.find('h3', text='Заведующий кафедрой:') # поиск заголовка

    name = title.find_next_sibling() # сразу после заголовка находится элемент span, в котором указаны ФИО и учёная степень
    name = name.span.text.split(',')[0] # берём только ФИО (указано до запятой)
    data['head_name'] = name # добавляем в словарь

    # для добавления e-mail необходимо перебрать элементы таблицы с информацией о сотрудниках кафедры,
    # найти в ней ФИО зав. кафедры, перейти по ссылке и спарсить информацию
    email = '' # e-mail по умолчанию пустой, т.к. его может не быть на странице

    table = bs.find('table', {'class': 'table_good'}) # таблица - список сотрудников кафедры
    for el in table.findAll('a'): # перебор всех ссылок
        teacher = el.text.strip() # удаление пробельных символов в ссылке
        if teacher == data['head_name']: # если ФИО в строке таблицы совпадает с ФИО зав.кафедры, то переходим к получению e-mail
            link = el.get('href') # получаем ссылку
            # например, https://atlas.herzen.spb.ru/teacher.php?id=8756

            html = requests.get(ATLAS_DOMAIN + link).text # получение кода страницы
            bs = BeautifulSoup(html, 'html.parser') # инициализация BeautifulSoup для парсинга страницы
            title = bs.find('h3', text='E-mail:') # поиск заголовка
            if title: # если заголовок найден
                email = title.find_next_sibling() # сразу после заголовка находится элемент span, в котором указан e-mail
                email = email.span.a.text # получаем текст (e-mail является ссылкой на странице)
    
    data['email'] = email # добавляем в словарь
    
    return data


def get_institutes_data(institutes_list):
    # получение информации о кафедрах и её добавление в исходный список институтов
    # информация получается со страницы https://atlas.herzen.spb.ru/faculty.php
    html = requests.get('https://atlas.herzen.spb.ru/faculty.php').text # получение кода страницы

    bs = BeautifulSoup(html, 'html.parser') # инициализация BeautifulSoup для парсинга страницы
    namesList = bs.find('ul', {'class': 'list'}) # ссылки указаны в списке (ul)
    names = namesList.findAll('li', {'class': 'list'}, recursive=False) # каждый элемент списка - li - ссылка, при нажатии на которую открывается список кафедр
    # recursive=False для того, чтобы выбрать только институты (элементы внешнего списка), т.е. чтобы не попали указанные внутри списка институтов (ul) списки кафедр (ul)

    institutes = [] # список институтов
    
    for el in names: # перебираем все элементы списка
        name = el.find('a', {'class': 'alist'}) # поиск ссылки в элементе списка

        data = dict() # информация об институте
        for institute in institutes_list: # поиск информации об институте (название и ссылка на страницу) из исходного списка институтов
            # в найденный элемент будет добавлена информация кафедрах
            if institute['institute_name'].lower() == name.text:
                data = institute
                break
        if not data:
            continue

        data['dep_list'] = [] # список кафедр
        
        depsList = el.find('ul', {'class': 'list'}) # получаем список кафедр (ul)
        deps = depsList.findAll('li', {'class': 'list'}) # перебираем все его элементы
        for dept in deps:
            dep = dept.find('a', {'class': 'alist'}) # получаем ссылку (элемент a) на страницу кафедры
            dep_name = dep.text.strip() # удаление пробельных символов
            link = dep.get('href') # берём ссылка на страницу
            if link.startswith('faculty_opop.php'): # ссылка "список ОПОП" не ведёт на страницу кафедры, пропускаем её
                continue

            head_info = get_head_info(link) # функция для получения информации о зав.кафедры
            # добавляем информацию о кафедре в список
            data['dep_list'].append({
                'dep_name': dep_name,
                'head_name': head_info['head_name'],
                'email': head_info['email']
            })

        institutes.append(data) # добавляем полную информацию об институте (с добавленным списком кафедр) в список

    return institutes


institutes_list = get_institutes_list()
print(institutes_list)

# вывод информации в отформатированном виде
import json

institutes_data = get_institutes_data(institutes_list)
print(json.dumps(institutes_data, indent=4, ensure_ascii=True))