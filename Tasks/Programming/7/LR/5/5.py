# -*- coding: utf-8 -*-
"""lab5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/120XYZVb0imFFFC8FACT41k165seqpe01
"""

import requests
import nltk

from collections import Counter

# дополнения для nltk
nltk.download('punkt') # для токенизации текста
nltk.download('averaged_perceptron_tagger') # для частеречной разметки

# загрузка текста
text = requests.get('https://gist.githubusercontent.com/nzhukov/b66c831ea88b4e5c4a044c952fb3e1ae/raw/7935e52297e2e85933e41d1fd16ed529f1e689f5/A%20Brief%20History%20of%20the%20Web.txt').text

tokens = nltk.word_tokenize(text) # токенизация текста
print(tokens)

tags = nltk.pos_tag(tokens) # частеречная разметка
print(tags)

parts = [part for word, part in tags] # получаем список частей речи (из списка пар "слово - часть речи")

# подсчёт самых встречаемых частей речи в тексте
counter = Counter(parts)

for part, count in counter.most_common(5):
    print(part, count)

print('-' * 50)

# список тегов: https://www.guru99.com/pos-tagging-chunking-nltk.html
# теги учитывают отдельно число, время глагола, степень сравнения прилагательных и т.д.
# например, NN - существительное единственного числа, NNS - существительное множественного числа
# их можно сгруппировать, оставив только первые 2 символа (NN - существительные, VB - глаголы и т.д.)

parts = [part[:2] for word, part in tags] # получаем список частей речи (из списка пар "слово - часть речи")
# оставляем только 2 символа

# подсчёт самых встречаемых частей речи в тексте
counter = Counter(parts)

for part, count in counter.most_common(5):
    print(part, count)

print('-' * 50)

names = {'NN': 'nouns', 'VB': 'verbs', 'IN': 'prepositions', 'DT': 'determiners', 'JJ': 'adjectives'} # части речи

# вывод того же списка с названиями частей речи вместо тегов
for part, count in counter.most_common(5):
    print(names[part], count)