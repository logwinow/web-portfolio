# -*- coding: utf-8 -*-
"""Шумякин И.С. ЛР 6. Обработка текста на Python / NLP with Python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h1NPTxgqiym9yVTi-El_9mkEQA_LF_0S

# Обработка текста на Python / NLP with Python 

Примерно то, что требуется в задании, продемонстрировано в статье на хабре по [ссылке](https://habr.com/ru/post/517410/).

Продемонстрируем некоторые отдельные вопросы, которые возможно возникнут при выполнении задания.

## Установка зависимостей в колабе

Необходимо использовать магические ячейки и установить некоторые библиотеки, которых в базовом наборе библиотек колаба, нет.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%sh
# 
# pip install yargy # для обработки текста 
# pip install natasha # для обработки текста 
# pip install wordcloud # для создания изображения с облаком тегов

"""## Парсинг и обработка информации со страниц сайта

Можно использовать механизм, используемый в прошлой ЛР (BeautifulSoup) или что-то другое.

Предлагается использовать регулярные выражения, поскольку все новости на сайте ведут на подраздел ```/news/*```

Извлекаем ссылки на конкретные новости и открываем каждую страницу аналогичным образом получаем текст новости полностью.

Не забыть выполнить предварительную очистку и токенизацию текста по словам. После очистки текста требуется положить его в отдельный текстовый файл или сохранить в строковую переменную. В конце — склеить весь текст в единый текстовый файл. 

Затем, используя [библиотеку natasha](https://natasha.github.io/ner/) извлекаем имена упоминаемых в тексте людей и добавляем их в json-файл. 
Далее, для имён требуется сериализовать данные из json-файла, убрав имена и оставив только фамилии. После этого сделать из фамилий текст и визуализировать информацию о статистики встречаемости в виде облака тегов. 
Аналогично, сделать облако тегов для ключевых понятий. 
"""

import re # для работы с регулярными выражениями
import requests # для выполнения веб-запросов
from bs4 import BeautifulSoup # для парсинга страниц

res = requests.get('https://www.herzen.spb.ru/main/news/') # запрос страницы
html = res.text # получение кода страницы

# поиск всех ссылок с помощью регулярного выражения
# пример ссылок: /news/20-11-2022/, /news/19-11-2022_1/, /news/10-12-2018_10/
# \d - любая цифра, {2} - кол-во цифр, (?:_\d+)? - часть с символом _ есть не во всех ссылках
pattern = r'\/news\/\d{2}-\d{2}-\d{4}(?:_\d+)?\/'
links = re.findall(pattern, html, re.MULTILINE) # поиск с флагом MULTILINE, т.к. строк много
print(links)

with open('buffer.txt', 'w') as buffer_file: # сохраним текст новостей в файл
    for link in links[:50]: # ссылок очень много, выберем только часть из них
        res = requests.get('https://www.herzen.spb.ru' + link) # запрос страницы
        html = res.text  # получение кода страницы

        bs = BeautifulSoup(html, 'html.parser') # парсинг страницы
        el = bs.find('td', class_='blockwh') # выбираем ячейку таблицы с классом blockwh
        news_block = el.findAll('div')[-1] # текст новости находится в последнем блоке div этой ячейки таблицы
        text = news_block.text # получаем текст из данного блока
        text = text.strip() # удаляем пробельные символы
        buffer_file.write(text + '\n') # записываем текст во временный файл (новая строка добавляется, чтобы тексты не "склеивались")

import nltk # пакет для обработки естественного языка
nltk.download('stopwords') # скачиваем пакет со списком стоп-слов

from nltk.corpus import stopwords # стоп-слова

from natasha import (
    Segmenter, # сегментация и токенизация текста
    MorphVocab, # анализ морфологии
    NewsEmbedding, # эмбеддинги текстов новостей
    NewsMorphTagger, # анализ морфологии текстов новостей
    NewsSyntaxParser, # анализ синтаксиса текстов новостей
    NewsNERTagger, # извлечение именованных сущностей
    PER, # константа, обозначающая имя собственное
    NamesExtractor, # извлечение имён
    Doc # документ, к которому применяются обработчики
)

with open('buffer.txt', 'r') as f: # чтение сохранённых текстов новостей из временного файла
    text = f.read()

text = text.replace('им. А. И. Герцена', '') # имеет смысл исключить из текста, т.к. все новости так или иначе относятся к университету
text = text.replace('РГПУ', '')

segmenter = Segmenter() # сегментация текста
morph_vocab = MorphVocab() # анализ морфологии

emb = NewsEmbedding() # эмбеддинги текстов новостей
morph_tagger = NewsMorphTagger(emb) # анализ морфологии текстов новостей
syntax_parser = NewsSyntaxParser(emb)  # анализ синтаксиса текстов новостей
ner_tagger = NewsNERTagger(emb) # извлечение именованных сущностей
names_extractor = NamesExtractor(morph_vocab) # извлечение имён

doc = Doc(text)
doc.segment(segmenter) # сегментация текста
doc.tag_morph(morph_tagger) # анализ морфологии
doc.parse_syntax(syntax_parser) # анализ синтаксиса

doc.tag_ner(ner_tagger) # извлечение именованных сущностей

names = [] # список имён
for span in doc.spans: # цикл по всем фразам текста
    span.normalize(morph_vocab) # нормализация сущности (например, университета -> университет)
    if span.type == PER: # если тип сущности - имя собственное
        span.extract_fact(names_extractor) # извлекаем имя
        names.append(span.normal) # добавляем имя в нормальной форме в список

words = [] # список слов
russian_stopwords = stopwords.words('russian') + ['который', 'это'] # добавляем стоп-слова к тем, которые есть в пакете nltk
punctuation = '.,!?"«»—():;+-'
for token in doc.tokens: # цикл по всем токенам
    token.lemmatize(morph_vocab) # лемматизация
    if token.lemma not in russian_stopwords: # если слово не входит в список стоп-слов
        if token.lemma not in punctuation: # если токен не является знаком препинания
            words.append(token.lemma) # добавляем в список слов

"""## Построение изображения с облаком тегов на Python"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

from collections import Counter # для подсчёта кол-ва повторений

from wordcloud import WordCloud # для построения облака слов
import matplotlib.pyplot as plt # для построения графиков

# функция generate из WordCloud принимает строку, но при соединеии имён в строку каждое слово будет учитываться отдельно
# но имя нужно учитывать полностью
# поэтому используем функцию generate_from_frequencies, которой передаём словарь с именами и кол-вом повторений в тексте
names_count = Counter(names) # подсчёт кол-ва повторений
wordcloud = WordCloud().generate_from_frequencies(names_count) # создание облака слов
plt.imshow(wordcloud, interpolation='bilinear') # построение изображения
plt.axis("off") # отключение отображения осей

words_count = Counter(words)
wordcloud = WordCloud().generate_from_frequencies(words_count)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")